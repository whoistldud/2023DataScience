{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed89cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 파일은 처음 project_data1, 2, 3 파일을 만들 때 사용한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f8f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f996cb9",
   "metadata": {},
   "source": [
    "### 1. '하이퍼클로바X' 관련 기사 크롤링하기 (11월 1일 ~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbeff66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색어 설정 : 하이퍼클로바X, 클로바X, 네이버 큐(CUE)\n",
    "keyword = '네이버 하이퍼클로바X'\n",
    "news_data = []\n",
    "\n",
    "# 크롤링 기간 설정 (2023.11.01 ~ 2023.11.10)\n",
    "crowl_url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=2023.11.01&de=2023.11.10&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20231105to20231105&is_sug_officeid=0&office_category=0&service_area=0\"\n",
    "\n",
    "# 브라우저 열기 & 웹페이지 접속\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(crowl_url)\n",
    "time.sleep(2)\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "# 크롤링해야 하는 기사 링크 담아놓을 리스트\n",
    "news_link = []\n",
    "\n",
    "# ainfo = 결과 기사의 언론사, '네이버뉴스' 태그 리스트\n",
    "ainfo = browser.find_elements(\"css selector\", \"ul.list_news li.bx div.info_group a.info\")\n",
    "\n",
    "# 네이버뉴스 달려있는 기사링크만 추출!\n",
    "for i,a in enumerate(ainfo):\n",
    "    if a.text == '네이버뉴스':\n",
    "        link = soup.select(\"div.info_group a.info\")[i]['href']\n",
    "        news_link.append(link)\n",
    "\n",
    "# 다음 페이지 버튼이 활성화되어 있는 경우\n",
    "# = 다음 페이지가 존재하는 경우\n",
    "# = is_next_page 값이 'false'인 경우\n",
    "is_next_page = soup.select(\"a.btn_next\")[0]['aria-disabled']\n",
    "btn_next = browser.find_elements(\"css selector\", \"a.btn_next\")[0]\n",
    "\n",
    "while True:\n",
    "\n",
    "    if is_next_page == 'false':\n",
    "        btn_next.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        ainfo = browser.find_elements(\"css selector\", \"ul.list_news li.bx div.info_group a.info\")\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        for i,a in enumerate(ainfo):\n",
    "            if a.text == '네이버뉴스':\n",
    "                link = soup.select(\"div.info_group a.info\")[i]['href']\n",
    "                news_link.append(link)\n",
    "\n",
    "        is_next_page = soup.select(\"a.btn_next\")[0]['aria-disabled']\n",
    "        btn_next = browser.find_elements(\"css selector\", \"a.btn_next\")[0]\n",
    "\n",
    "    elif is_next_page == 'true':\n",
    "        break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "# 링크 하나씩 넣어서 크롤링 시작\n",
    "for news in news_link:\n",
    "    browser.get(news)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 기사 제목\n",
    "    title = browser.find_elements(\"css selector\", \"h2#title_area\")[0].text\n",
    "\n",
    "    # 언론사\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    company = soup.select(\"a.media_end_head_top_logo._LAZY_LOADING_ERROR_HIDE img\")[0]['title']\n",
    "\n",
    "    # 최종입력시간 / 입력시간\n",
    "    write_time = browser.find_elements(\"css selector\", \"div.media_end_head_info_datestamp_bunch\")\n",
    "    # len(write_time) == 2 이면 입력시간, 최종입력시간 둘다, len(write_time) == 1 이면 입력시간 = 최종입력시간\n",
    "    if len(write_time) == 1:\n",
    "        first = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "        enter = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "    elif len(write_time) == 2:\n",
    "        first = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "        enter = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_MODIFY_DATE_TIME\")[0]['data-modify-date-time']\n",
    "    \n",
    "    news_content = browser.find_elements(\"css selector\", \"div#newsct_article\")[0].text.replace(\"\\n\", ' ').replace(\"\\'\", \"\")\n",
    "\n",
    "    data = [title, company, enter, first, news, news_content]\n",
    "    news_data.append(data)\n",
    "\n",
    "# 겹치는 기사는 제거\n",
    "tuple_origin = [tuple(i) for i in news_data]\n",
    "unique_data = [list(j) for j in set(tuple_origin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd8560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95d3a708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>언론사</th>\n",
       "      <th>최종입력시간</th>\n",
       "      <th>입력시간</th>\n",
       "      <th>URL</th>\n",
       "      <th>본문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'K콘텐츠' 넘어 'K플랫폼' 쏟아지길 [기자수첩]</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>2023-11-16 11:00:06</td>\n",
       "      <td>2023-11-09 16:17:02</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/014/000...</td>\n",
       "      <td>[파이낸셜뉴스] \"오징어게임이 넷플릭스에서 1위를 하고 BTS(방탄소년단)가 빌보드...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[인터뷰] '링크' 최찬열 대표 \"아이언맨의 '자비스' 만들려면…\"</td>\n",
       "      <td>주간조선</td>\n",
       "      <td>2023-11-14 10:31:01</td>\n",
       "      <td>2023-11-07 04:01:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/053/000...</td>\n",
       "      <td>photo 이신영 영상미디어 기자   특정 사유에 따른 이혼소송 승소 가능성 적합한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[IS현장] 자연이 식히고 로봇이 나르고…네이버가 세종에 세운 최첨단 데이터 방주</td>\n",
       "      <td>일간스포츠</td>\n",
       "      <td>2023-11-10 20:20:01</td>\n",
       "      <td>2023-11-10 07:00:04</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/241/000...</td>\n",
       "      <td>아시아 최대 규모 자랑하는 각 세종 축구장 41개·국립중앙도서관 100만배 외기로 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI 기술패권 경쟁에 뛰어든 韓 …순항 시작한 네이버, 잠시 숨고른 카카오</td>\n",
       "      <td>디지털데일리</td>\n",
       "      <td>2023-11-10 18:17:01</td>\n",
       "      <td>2023-11-10 18:17:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/138/000...</td>\n",
       "      <td>[ⓒ 네이버·카카오]   [디지털데일리 이나연 기자] 출발선은 같았지만, 속도는 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[DD퇴근길] KT-LGU+ 누가 2위일까…넥슨 독주에 크래프톤 약진</td>\n",
       "      <td>디지털데일리</td>\n",
       "      <td>2023-11-10 17:31:05</td>\n",
       "      <td>2023-11-10 17:31:05</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/138/000...</td>\n",
       "      <td>디지털데일리가 퇴근 즈음해서 읽을 수 있는 [DD퇴근길] 코너를 마련했습니다. 하루...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>삼정KPMG-오브젠, 초거대 AI 기반 비즈니스 혁신 '맞손'</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>2023-11-02 08:32:03</td>\n",
       "      <td>2023-11-02 08:32:03</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/014/000...</td>\n",
       "      <td>양사 협력으로 초거대 AI 활용한 새 비즈니스 기회 창출 기대 생성형 AI 활용한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3분기 실적에 네이버 '웃고' 카카오 '울고'</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>2023-11-01 18:08:15</td>\n",
       "      <td>2023-11-01 16:02:08</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/003/001...</td>\n",
       "      <td>경기 침체 지속되며 네카오 광고 성장 둔화 네이버, 커머스 견조·비용 통제로 선방…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>이영해 울산시의원, AI 활용해 5분 자유발언 진행</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>2023-11-01 11:31:16</td>\n",
       "      <td>2023-11-01 11:30:47</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/001/001...</td>\n",
       "      <td>수돗물 이용 활성화를 위한 울산시 인식 전환 주문   AI 이용 5분 자유발언을 한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>네카오 3분기 실적도 엇갈릴 듯…양대 플랫폼 체제 '흔들'</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>2023-11-01 09:04:12</td>\n",
       "      <td>2023-11-01 09:03:14</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/001/001...</td>\n",
       "      <td>작년비 분기별 영업익 네이버 증가·카카오 감소 지속 하이퍼클로바X로 성장 견인 VS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>KT 초거대 AI ‘믿음’ 출시…“B2B 시장 잡겠다”</td>\n",
       "      <td>중앙일보</td>\n",
       "      <td>2023-11-01 00:04:05</td>\n",
       "      <td>2023-11-01 00:03:00</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "      <td>KT가 초거대 인공지능(AI) ‘믿음’을 공식 출시하며, AI B2B(기업간 거래)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                제목     언론사  \\\n",
       "0                     'K콘텐츠' 넘어 'K플랫폼' 쏟아지길 [기자수첩]  파이낸셜뉴스   \n",
       "1            [인터뷰] '링크' 최찬열 대표 \"아이언맨의 '자비스' 만들려면…\"    주간조선   \n",
       "2    [IS현장] 자연이 식히고 로봇이 나르고…네이버가 세종에 세운 최첨단 데이터 방주   일간스포츠   \n",
       "3        AI 기술패권 경쟁에 뛰어든 韓 …순항 시작한 네이버, 잠시 숨고른 카카오  디지털데일리   \n",
       "4           [DD퇴근길] KT-LGU+ 누가 2위일까…넥슨 독주에 크래프톤 약진  디지털데일리   \n",
       "..                                             ...     ...   \n",
       "147             삼정KPMG-오브젠, 초거대 AI 기반 비즈니스 혁신 '맞손'  파이낸셜뉴스   \n",
       "148                      3분기 실적에 네이버 '웃고' 카카오 '울고'     뉴시스   \n",
       "149                   이영해 울산시의원, AI 활용해 5분 자유발언 진행    연합뉴스   \n",
       "150               네카오 3분기 실적도 엇갈릴 듯…양대 플랫폼 체제 '흔들'    연합뉴스   \n",
       "151                 KT 초거대 AI ‘믿음’ 출시…“B2B 시장 잡겠다”    중앙일보   \n",
       "\n",
       "                  최종입력시간                 입력시간  \\\n",
       "0    2023-11-16 11:00:06  2023-11-09 16:17:02   \n",
       "1    2023-11-14 10:31:01  2023-11-07 04:01:01   \n",
       "2    2023-11-10 20:20:01  2023-11-10 07:00:04   \n",
       "3    2023-11-10 18:17:01  2023-11-10 18:17:01   \n",
       "4    2023-11-10 17:31:05  2023-11-10 17:31:05   \n",
       "..                   ...                  ...   \n",
       "147  2023-11-02 08:32:03  2023-11-02 08:32:03   \n",
       "148  2023-11-01 18:08:15  2023-11-01 16:02:08   \n",
       "149  2023-11-01 11:31:16  2023-11-01 11:30:47   \n",
       "150  2023-11-01 09:04:12  2023-11-01 09:03:14   \n",
       "151  2023-11-01 00:04:05  2023-11-01 00:03:00   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://n.news.naver.com/mnews/article/014/000...   \n",
       "1    https://n.news.naver.com/mnews/article/053/000...   \n",
       "2    https://n.news.naver.com/mnews/article/241/000...   \n",
       "3    https://n.news.naver.com/mnews/article/138/000...   \n",
       "4    https://n.news.naver.com/mnews/article/138/000...   \n",
       "..                                                 ...   \n",
       "147  https://n.news.naver.com/mnews/article/014/000...   \n",
       "148  https://n.news.naver.com/mnews/article/003/001...   \n",
       "149  https://n.news.naver.com/mnews/article/001/001...   \n",
       "150  https://n.news.naver.com/mnews/article/001/001...   \n",
       "151  https://n.news.naver.com/mnews/article/025/000...   \n",
       "\n",
       "                                                    본문  \n",
       "0    [파이낸셜뉴스] \"오징어게임이 넷플릭스에서 1위를 하고 BTS(방탄소년단)가 빌보드...  \n",
       "1    photo 이신영 영상미디어 기자   특정 사유에 따른 이혼소송 승소 가능성 적합한...  \n",
       "2    아시아 최대 규모 자랑하는 각 세종 축구장 41개·국립중앙도서관 100만배 외기로 ...  \n",
       "3    [ⓒ 네이버·카카오]   [디지털데일리 이나연 기자] 출발선은 같았지만, 속도는 달...  \n",
       "4    디지털데일리가 퇴근 즈음해서 읽을 수 있는 [DD퇴근길] 코너를 마련했습니다. 하루...  \n",
       "..                                                 ...  \n",
       "147  양사 협력으로 초거대 AI 활용한 새 비즈니스 기회 창출 기대 생성형 AI 활용한 ...  \n",
       "148  경기 침체 지속되며 네카오 광고 성장 둔화 네이버, 커머스 견조·비용 통제로 선방…...  \n",
       "149  수돗물 이용 활성화를 위한 울산시 인식 전환 주문   AI 이용 5분 자유발언을 한...  \n",
       "150  작년비 분기별 영업익 네이버 증가·카카오 감소 지속 하이퍼클로바X로 성장 견인 VS...  \n",
       "151  KT가 초거대 인공지능(AI) ‘믿음’을 공식 출시하며, AI B2B(기업간 거래)...  \n",
       "\n",
       "[152 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(unique_data)\n",
    "df.columns = ['제목', '언론사', '최종입력시간', '입력시간', 'URL', '본문']\n",
    "df = df.sort_values(by=['최종입력시간', '입력시간'], ascending=[False, False]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1e2f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이후에 new 컬럼이 필요해져서 아래 코드로 'new' 컬럼을 생성하여 저장하였습니다. \n",
    "df.insert(0, 'new', ' ')\n",
    "df.to_excel('./project_data1.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71908c1",
   "metadata": {},
   "source": [
    "### 2. '클로바X' 관련 기사 크롤링하기 (11월 1일 ~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb322491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "\n",
    "# 검색어 설정 : 하이퍼클로바X, 클로바X, 네이버 큐(CUE)\n",
    "keyword = '네이버 클로바X'\n",
    "news_data = []\n",
    "\n",
    "# 크롤링 기간 설정 (2023.11.01 ~ 2023.11.10)\n",
    "crowl_url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=2023.11.01&de=2023.11.10&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20231105to20231105&is_sug_officeid=0&office_category=0&service_area=0\"\n",
    "\n",
    "# 브라우저 열기 & 웹페이지 접속\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(crowl_url)\n",
    "time.sleep(2)\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "# 크롤링해야 하는 기사 링크 담아놓을 리스트\n",
    "news_link = []\n",
    "\n",
    "# ainfo = 결과 기사의 언론사, '네이버뉴스' 태그 리스트\n",
    "ainfo = browser.find_elements(\"css selector\", \"ul.list_news li.bx div.info_group a.info\")\n",
    "\n",
    "# 네이버뉴스 달려있는 기사링크만 추출!\n",
    "for i,a in enumerate(ainfo):\n",
    "    if a.text == '네이버뉴스':\n",
    "        link = soup.select(\"div.info_group a.info\")[i]['href']\n",
    "        news_link.append(link)\n",
    "\n",
    "# 다음 페이지 버튼이 활성화되어 있는 경우\n",
    "# = 다음 페이지가 존재하는 경우\n",
    "# = is_next_page 값이 'false'인 경우\n",
    "is_next_page = soup.select(\"a.btn_next\")[0]['aria-disabled']\n",
    "btn_next = browser.find_elements(\"css selector\", \"a.btn_next\")[0]\n",
    "\n",
    "while True:\n",
    "\n",
    "    if is_next_page == 'false':\n",
    "        btn_next.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        ainfo = browser.find_elements(\"css selector\", \"ul.list_news li.bx div.info_group a.info\")\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        for i,a in enumerate(ainfo):\n",
    "            if a.text == '네이버뉴스':\n",
    "                link = soup.select(\"div.info_group a.info\")[i]['href']\n",
    "                news_link.append(link)\n",
    "\n",
    "        is_next_page = soup.select(\"a.btn_next\")[0]['aria-disabled']\n",
    "        btn_next = browser.find_elements(\"css selector\", \"a.btn_next\")[0]\n",
    "\n",
    "    elif is_next_page == 'true':\n",
    "        break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "# 링크 하나씩 넣어서 크롤링 시작\n",
    "for news in news_link:\n",
    "    browser.get(news)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 기사 제목\n",
    "    title = browser.find_elements(\"css selector\", \"h2#title_area\")[0].text\n",
    "\n",
    "    # 언론사\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    company = soup.select(\"a.media_end_head_top_logo._LAZY_LOADING_ERROR_HIDE img\")[0]['title']\n",
    "\n",
    "    # 최종 입력시간\n",
    "    write_time = browser.find_elements(\"css selector\", \"div.media_end_head_info_datestamp_bunch\")\n",
    "    # len(write_time) == 2 이면 수정한 시간 write_time[1]을 가져오고, len(write_time) == 1 이면 입력시간\n",
    "    if len(write_time) == 1:\n",
    "        first = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "        enter = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "    elif len(write_time) == 2:\n",
    "        first = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "        enter = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_MODIFY_DATE_TIME\")[0]['data-modify-date-time']\n",
    "    \n",
    "    news_content = browser.find_elements(\"css selector\", \"div#newsct_article\")[0].text.replace(\"\\n\", ' ').replace(\"\\'\", \"\")\n",
    "\n",
    "    data = [title, company, enter, first, news, news_content]\n",
    "    news_data.append(data)\n",
    "\n",
    "# 겹치는 기사는 제거\n",
    "tuple_origin = [tuple(i) for i in news_data]\n",
    "unique_data = [list(j) for j in set(tuple_origin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7098d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12542fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>언론사</th>\n",
       "      <th>최종입력시간</th>\n",
       "      <th>입력시간</th>\n",
       "      <th>URL</th>\n",
       "      <th>본문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'K콘텐츠' 넘어 'K플랫폼' 쏟아지길 [기자수첩]</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>2023-11-16 11:00:06</td>\n",
       "      <td>2023-11-09 16:17:02</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/014/000...</td>\n",
       "      <td>[파이낸셜뉴스] \"오징어게임이 넷플릭스에서 1위를 하고 BTS(방탄소년단)가 빌보드...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[인터뷰] '링크' 최찬열 대표 \"아이언맨의 '자비스' 만들려면…\"</td>\n",
       "      <td>주간조선</td>\n",
       "      <td>2023-11-14 10:31:01</td>\n",
       "      <td>2023-11-07 04:01:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/053/000...</td>\n",
       "      <td>photo 이신영 영상미디어 기자   특정 사유에 따른 이혼소송 승소 가능성 적합한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[IS현장] 자연이 식히고 로봇이 나르고…네이버가 세종에 세운 최첨단 데이터 방주</td>\n",
       "      <td>일간스포츠</td>\n",
       "      <td>2023-11-10 20:20:01</td>\n",
       "      <td>2023-11-10 07:00:04</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/241/000...</td>\n",
       "      <td>아시아 최대 규모 자랑하는 각 세종 축구장 41개·국립중앙도서관 100만배 외기로 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AI 기술패권 경쟁에 뛰어든 韓 …순항 시작한 네이버, 잠시 숨고른 카카오</td>\n",
       "      <td>디지털데일리</td>\n",
       "      <td>2023-11-10 18:17:01</td>\n",
       "      <td>2023-11-10 18:17:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/138/000...</td>\n",
       "      <td>[ⓒ 네이버·카카오]   [디지털데일리 이나연 기자] 출발선은 같았지만, 속도는 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[DD퇴근길] KT-LGU+ 누가 2위일까…넥슨 독주에 크래프톤 약진</td>\n",
       "      <td>디지털데일리</td>\n",
       "      <td>2023-11-10 17:31:05</td>\n",
       "      <td>2023-11-10 17:31:05</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/138/000...</td>\n",
       "      <td>디지털데일리가 퇴근 즈음해서 읽을 수 있는 [DD퇴근길] 코너를 마련했습니다. 하루...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>삼정KPMG-오브젠, 초거대 AI 기반 비즈니스 혁신 '맞손'</td>\n",
       "      <td>파이낸셜뉴스</td>\n",
       "      <td>2023-11-02 08:32:03</td>\n",
       "      <td>2023-11-02 08:32:03</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/014/000...</td>\n",
       "      <td>양사 협력으로 초거대 AI 활용한 새 비즈니스 기회 창출 기대 생성형 AI 활용한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3분기 실적에 네이버 '웃고' 카카오 '울고'</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>2023-11-01 18:08:15</td>\n",
       "      <td>2023-11-01 16:02:08</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/003/001...</td>\n",
       "      <td>경기 침체 지속되며 네카오 광고 성장 둔화 네이버, 커머스 견조·비용 통제로 선방…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>이영해 울산시의원, AI 활용해 5분 자유발언 진행</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>2023-11-01 11:31:16</td>\n",
       "      <td>2023-11-01 11:30:47</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/001/001...</td>\n",
       "      <td>수돗물 이용 활성화를 위한 울산시 인식 전환 주문   AI 이용 5분 자유발언을 한...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>네카오 3분기 실적도 엇갈릴 듯…양대 플랫폼 체제 '흔들'</td>\n",
       "      <td>연합뉴스</td>\n",
       "      <td>2023-11-01 09:04:12</td>\n",
       "      <td>2023-11-01 09:03:14</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/001/001...</td>\n",
       "      <td>작년비 분기별 영업익 네이버 증가·카카오 감소 지속 하이퍼클로바X로 성장 견인 VS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>KT 초거대 AI ‘믿음’ 출시…“B2B 시장 잡겠다”</td>\n",
       "      <td>중앙일보</td>\n",
       "      <td>2023-11-01 00:04:05</td>\n",
       "      <td>2023-11-01 00:03:00</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "      <td>KT가 초거대 인공지능(AI) ‘믿음’을 공식 출시하며, AI B2B(기업간 거래)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                제목     언론사  \\\n",
       "0                     'K콘텐츠' 넘어 'K플랫폼' 쏟아지길 [기자수첩]  파이낸셜뉴스   \n",
       "1            [인터뷰] '링크' 최찬열 대표 \"아이언맨의 '자비스' 만들려면…\"    주간조선   \n",
       "2    [IS현장] 자연이 식히고 로봇이 나르고…네이버가 세종에 세운 최첨단 데이터 방주   일간스포츠   \n",
       "3        AI 기술패권 경쟁에 뛰어든 韓 …순항 시작한 네이버, 잠시 숨고른 카카오  디지털데일리   \n",
       "4           [DD퇴근길] KT-LGU+ 누가 2위일까…넥슨 독주에 크래프톤 약진  디지털데일리   \n",
       "..                                             ...     ...   \n",
       "149             삼정KPMG-오브젠, 초거대 AI 기반 비즈니스 혁신 '맞손'  파이낸셜뉴스   \n",
       "150                      3분기 실적에 네이버 '웃고' 카카오 '울고'     뉴시스   \n",
       "151                   이영해 울산시의원, AI 활용해 5분 자유발언 진행    연합뉴스   \n",
       "152               네카오 3분기 실적도 엇갈릴 듯…양대 플랫폼 체제 '흔들'    연합뉴스   \n",
       "153                 KT 초거대 AI ‘믿음’ 출시…“B2B 시장 잡겠다”    중앙일보   \n",
       "\n",
       "                  최종입력시간                 입력시간  \\\n",
       "0    2023-11-16 11:00:06  2023-11-09 16:17:02   \n",
       "1    2023-11-14 10:31:01  2023-11-07 04:01:01   \n",
       "2    2023-11-10 20:20:01  2023-11-10 07:00:04   \n",
       "3    2023-11-10 18:17:01  2023-11-10 18:17:01   \n",
       "4    2023-11-10 17:31:05  2023-11-10 17:31:05   \n",
       "..                   ...                  ...   \n",
       "149  2023-11-02 08:32:03  2023-11-02 08:32:03   \n",
       "150  2023-11-01 18:08:15  2023-11-01 16:02:08   \n",
       "151  2023-11-01 11:31:16  2023-11-01 11:30:47   \n",
       "152  2023-11-01 09:04:12  2023-11-01 09:03:14   \n",
       "153  2023-11-01 00:04:05  2023-11-01 00:03:00   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://n.news.naver.com/mnews/article/014/000...   \n",
       "1    https://n.news.naver.com/mnews/article/053/000...   \n",
       "2    https://n.news.naver.com/mnews/article/241/000...   \n",
       "3    https://n.news.naver.com/mnews/article/138/000...   \n",
       "4    https://n.news.naver.com/mnews/article/138/000...   \n",
       "..                                                 ...   \n",
       "149  https://n.news.naver.com/mnews/article/014/000...   \n",
       "150  https://n.news.naver.com/mnews/article/003/001...   \n",
       "151  https://n.news.naver.com/mnews/article/001/001...   \n",
       "152  https://n.news.naver.com/mnews/article/001/001...   \n",
       "153  https://n.news.naver.com/mnews/article/025/000...   \n",
       "\n",
       "                                                    본문  \n",
       "0    [파이낸셜뉴스] \"오징어게임이 넷플릭스에서 1위를 하고 BTS(방탄소년단)가 빌보드...  \n",
       "1    photo 이신영 영상미디어 기자   특정 사유에 따른 이혼소송 승소 가능성 적합한...  \n",
       "2    아시아 최대 규모 자랑하는 각 세종 축구장 41개·국립중앙도서관 100만배 외기로 ...  \n",
       "3    [ⓒ 네이버·카카오]   [디지털데일리 이나연 기자] 출발선은 같았지만, 속도는 달...  \n",
       "4    디지털데일리가 퇴근 즈음해서 읽을 수 있는 [DD퇴근길] 코너를 마련했습니다. 하루...  \n",
       "..                                                 ...  \n",
       "149  양사 협력으로 초거대 AI 활용한 새 비즈니스 기회 창출 기대 생성형 AI 활용한 ...  \n",
       "150  경기 침체 지속되며 네카오 광고 성장 둔화 네이버, 커머스 견조·비용 통제로 선방…...  \n",
       "151  수돗물 이용 활성화를 위한 울산시 인식 전환 주문   AI 이용 5분 자유발언을 한...  \n",
       "152  작년비 분기별 영업익 네이버 증가·카카오 감소 지속 하이퍼클로바X로 성장 견인 VS...  \n",
       "153  KT가 초거대 인공지능(AI) ‘믿음’을 공식 출시하며, AI B2B(기업간 거래)...  \n",
       "\n",
       "[154 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(unique_data)\n",
    "df.columns = ['제목', '언론사', '최종입력시간', '입력시간', 'URL', '본문']\n",
    "df = df.sort_values(by=['최종입력시간', '입력시간'], ascending=[False, False]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e372a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, 'new', ' ')\n",
    "df.to_excel('./project_data2.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eff128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac092ec8",
   "metadata": {},
   "source": [
    "### 3. '큐' 관련 기사 크롤링하기 (11월 1일 ~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a77deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색어 설정 : 하이퍼클로바X, 클로바X, 네이버 큐(CUE)\n",
    "keyword = '네이버 큐'\n",
    "news_data = []\n",
    "\n",
    "# 크롤링 기간 설정 (2023.11.01 ~ 2023.11.10)\n",
    "crowl_url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds=2023.11.01&de=2023.11.10&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom20231105to20231105&is_sug_officeid=0&office_category=0&service_area=0\"\n",
    "\n",
    "# 브라우저 열기 & 웹페이지 접속\n",
    "browser = webdriver.Chrome()\n",
    "browser.get(crowl_url)\n",
    "time.sleep(2)\n",
    "\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "\n",
    "# 크롤링해야 하는 기사 링크 담아놓을 리스트\n",
    "news_link = []\n",
    "\n",
    "# ainfo = 결과 기사의 언론사, '네이버뉴스' 태그 리스트\n",
    "ainfo = browser.find_elements(\"css selector\", \"ul.list_news li.bx div.info_group a.info\")\n",
    "\n",
    "# 네이버뉴스 달려있는 기사링크만 추출!\n",
    "for i,a in enumerate(ainfo):\n",
    "    if a.text == '네이버뉴스':\n",
    "        link = soup.select(\"div.info_group a.info\")[i]['href']\n",
    "        news_link.append(link)\n",
    "\n",
    "# 다음 페이지 버튼이 활성화되어 있는 경우\n",
    "# = 다음 페이지가 존재하는 경우\n",
    "# = is_next_page 값이 'false'인 경우\n",
    "is_next_page = soup.select(\"a.btn_next\")[0]['aria-disabled']\n",
    "btn_next = browser.find_elements(\"css selector\", \"a.btn_next\")[0]\n",
    "\n",
    "while True:\n",
    "\n",
    "    if is_next_page == 'false':\n",
    "        btn_next.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        ainfo = browser.find_elements(\"css selector\", \"ul.list_news li.bx div.info_group a.info\")\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        for i,a in enumerate(ainfo):\n",
    "            if a.text == '네이버뉴스':\n",
    "                link = soup.select(\"div.info_group a.info\")[i]['href']\n",
    "                news_link.append(link)\n",
    "\n",
    "        is_next_page = soup.select(\"a.btn_next\")[0]['aria-disabled']\n",
    "        btn_next = browser.find_elements(\"css selector\", \"a.btn_next\")[0]\n",
    "\n",
    "    elif is_next_page == 'true':\n",
    "        break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "# 링크 하나씩 넣어서 크롤링 시작\n",
    "for news in news_link:\n",
    "    browser.get(news)\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 기사 제목\n",
    "    title = browser.find_elements(\"css selector\", \"h2#title_area\")[0].text\n",
    "\n",
    "    # 언론사\n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    company = soup.select(\"a.media_end_head_top_logo._LAZY_LOADING_ERROR_HIDE img\")[0]['title']\n",
    "\n",
    "    # 최종 입력시간\n",
    "    write_time = browser.find_elements(\"css selector\", \"div.media_end_head_info_datestamp_bunch\")\n",
    "    # len(write_time) == 2 이면 수정한 시간 write_time[1]을 가져오고, len(write_time) == 1 이면 입력시간\n",
    "    if len(write_time) == 1:\n",
    "        first = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "        enter = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "    elif len(write_time) == 2:\n",
    "        first = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME\")[0]['data-date-time']\n",
    "        enter = soup.select(\"span.media_end_head_info_datestamp_time._ARTICLE_MODIFY_DATE_TIME\")[0]['data-modify-date-time']\n",
    "    \n",
    "    news_content = browser.find_elements(\"css selector\", \"div#newsct_article\")[0].text.replace(\"\\n\", ' ').replace(\"\\'\", \"\")\n",
    "\n",
    "    data = [title, company, enter, first, news, news_content]\n",
    "    news_data.append(data)\n",
    "\n",
    "# 겹치는 기사는 제거\n",
    "tuple_origin = [tuple(i) for i in news_data]\n",
    "unique_data = [list(j) for j in set(tuple_origin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aa0ddee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8557f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>언론사</th>\n",
       "      <th>최종입력시간</th>\n",
       "      <th>입력시간</th>\n",
       "      <th>URL</th>\n",
       "      <th>본문</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI 기술패권 경쟁에 뛰어든 韓 …순항 시작한 네이버, 잠시 숨고른 카카오</td>\n",
       "      <td>디지털데일리</td>\n",
       "      <td>2023-11-10 18:17:01</td>\n",
       "      <td>2023-11-10 18:17:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/138/000...</td>\n",
       "      <td>[ⓒ 네이버·카카오]   [디지털데일리 이나연 기자] 출발선은 같았지만, 속도는 달...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘웅장한’ 네이버 데이터센터 각 세종, 협소하다고 느낀 까닭 [가봤어요]</td>\n",
       "      <td>이코노미스트</td>\n",
       "      <td>2023-11-10 15:39:01</td>\n",
       "      <td>2023-11-08 17:28:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/243/000...</td>\n",
       "      <td>‘아시아 최대’ 데이터센터 각 세종, 48개월 준비 끝에 운영 본격화 ‘각 춘천’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[사고]생성형 AI가 변화시킬 미래, 먼저 만나보세요[ECF for 2024]</td>\n",
       "      <td>이데일리</td>\n",
       "      <td>2023-11-10 13:17:01</td>\n",
       "      <td>2023-11-10 13:17:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/018/000...</td>\n",
       "      <td>제10회 이데일리 IT컨버전스포럼 개최 21일 오전 10시 여의도 FKI타워 컨퍼런...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국 출시 구글 생성형 인공지능 기반 검색 서비스가 미칠 영향은?</td>\n",
       "      <td>미디어오늘</td>\n",
       "      <td>2023-11-10 11:48:00</td>\n",
       "      <td>2023-11-10 11:40:01</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/006/000...</td>\n",
       "      <td>생성형 AI 검색 경쟁 나선 검색엔진들 환각현상 등 한계 있어 테스트 버전 보조적 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나란히 역대 최고 매출… 수익성은 네이버 ‘웃고’ 카카오 ‘울고’</td>\n",
       "      <td>세계일보</td>\n",
       "      <td>2023-11-09 20:18:06</td>\n",
       "      <td>2023-11-09 20:18:06</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/022/000...</td>\n",
       "      <td>양대 플랫폼 3분기 엇갈린 실적  네이버 매출 18.9%↑ 2조4453억 영업익도 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>[컨콜] 네이버 \"생성형 AI 검색 '큐:' 내년 모바일 적용\"</td>\n",
       "      <td>뉴시스</td>\n",
       "      <td>2023-11-03 09:22:05</td>\n",
       "      <td>2023-11-03 09:22:05</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/003/001...</td>\n",
       "      <td>11월부터 PC 통합 검색에 부분 적용 예정 내년 모바일 환경에 도입…멀티모달 기술...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>‘카카오’만 붙이면 성공하던 시대 끝나</td>\n",
       "      <td>매경이코노미</td>\n",
       "      <td>2023-11-02 21:02:03</td>\n",
       "      <td>2023-11-02 21:02:03</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/024/000...</td>\n",
       "      <td>카카오 주요 사업 어디로 ‘카톡’ 너마저…신사업·해외 공략 차질   2010년대 중...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>\"접속화면이 다 다르다고?\"…AI 검색 적용 '네이버 서비스 진화'</td>\n",
       "      <td>뉴스1</td>\n",
       "      <td>2023-11-02 16:38:01</td>\n",
       "      <td>2023-11-02 16:37:06</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/421/000...</td>\n",
       "      <td>\"AI 콘텐츠 추천 시스템에 하이퍼클로바X 더 많이 적용…추천 고도화\" 네이버 웹사...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>광고시장 위축됐는데…네이버 이익 확 늘어날것 같은 예감이, 왜?</td>\n",
       "      <td>매일경제</td>\n",
       "      <td>2023-11-02 16:32:00</td>\n",
       "      <td>2023-11-02 16:32:00</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "      <td>네이버, 실적 발표 D-1 광고 매출 성장 둔화 전망 광고수익 빈자리 커머스로 신규...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>KT 초거대 AI ‘믿음’ 출시…“B2B 시장 잡겠다”</td>\n",
       "      <td>중앙일보</td>\n",
       "      <td>2023-11-01 00:04:05</td>\n",
       "      <td>2023-11-01 00:03:00</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/025/000...</td>\n",
       "      <td>KT가 초거대 인공지능(AI) ‘믿음’을 공식 출시하며, AI B2B(기업간 거래)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             제목     언론사               최종입력시간  \\\n",
       "0     AI 기술패권 경쟁에 뛰어든 韓 …순항 시작한 네이버, 잠시 숨고른 카카오  디지털데일리  2023-11-10 18:17:01   \n",
       "1      ‘웅장한’ 네이버 데이터센터 각 세종, 협소하다고 느낀 까닭 [가봤어요]  이코노미스트  2023-11-10 15:39:01   \n",
       "2   [사고]생성형 AI가 변화시킬 미래, 먼저 만나보세요[ECF for 2024]    이데일리  2023-11-10 13:17:01   \n",
       "3          한국 출시 구글 생성형 인공지능 기반 검색 서비스가 미칠 영향은?   미디어오늘  2023-11-10 11:48:00   \n",
       "4          나란히 역대 최고 매출… 수익성은 네이버 ‘웃고’ 카카오 ‘울고’    세계일보  2023-11-09 20:18:06   \n",
       "..                                          ...     ...                  ...   \n",
       "58          [컨콜] 네이버 \"생성형 AI 검색 '큐:' 내년 모바일 적용\"     뉴시스  2023-11-03 09:22:05   \n",
       "59                        ‘카카오’만 붙이면 성공하던 시대 끝나  매경이코노미  2023-11-02 21:02:03   \n",
       "60        \"접속화면이 다 다르다고?\"…AI 검색 적용 '네이버 서비스 진화'     뉴스1  2023-11-02 16:38:01   \n",
       "61          광고시장 위축됐는데…네이버 이익 확 늘어날것 같은 예감이, 왜?    매일경제  2023-11-02 16:32:00   \n",
       "62               KT 초거대 AI ‘믿음’ 출시…“B2B 시장 잡겠다”    중앙일보  2023-11-01 00:04:05   \n",
       "\n",
       "                   입력시간                                                URL  \\\n",
       "0   2023-11-10 18:17:01  https://n.news.naver.com/mnews/article/138/000...   \n",
       "1   2023-11-08 17:28:01  https://n.news.naver.com/mnews/article/243/000...   \n",
       "2   2023-11-10 13:17:01  https://n.news.naver.com/mnews/article/018/000...   \n",
       "3   2023-11-10 11:40:01  https://n.news.naver.com/mnews/article/006/000...   \n",
       "4   2023-11-09 20:18:06  https://n.news.naver.com/mnews/article/022/000...   \n",
       "..                  ...                                                ...   \n",
       "58  2023-11-03 09:22:05  https://n.news.naver.com/mnews/article/003/001...   \n",
       "59  2023-11-02 21:02:03  https://n.news.naver.com/mnews/article/024/000...   \n",
       "60  2023-11-02 16:37:06  https://n.news.naver.com/mnews/article/421/000...   \n",
       "61  2023-11-02 16:32:00  https://n.news.naver.com/mnews/article/009/000...   \n",
       "62  2023-11-01 00:03:00  https://n.news.naver.com/mnews/article/025/000...   \n",
       "\n",
       "                                                   본문  \n",
       "0   [ⓒ 네이버·카카오]   [디지털데일리 이나연 기자] 출발선은 같았지만, 속도는 달...  \n",
       "1   ‘아시아 최대’ 데이터센터 각 세종, 48개월 준비 끝에 운영 본격화 ‘각 춘천’ ...  \n",
       "2   제10회 이데일리 IT컨버전스포럼 개최 21일 오전 10시 여의도 FKI타워 컨퍼런...  \n",
       "3   생성형 AI 검색 경쟁 나선 검색엔진들 환각현상 등 한계 있어 테스트 버전 보조적 ...  \n",
       "4   양대 플랫폼 3분기 엇갈린 실적  네이버 매출 18.9%↑ 2조4453억 영업익도 ...  \n",
       "..                                                ...  \n",
       "58  11월부터 PC 통합 검색에 부분 적용 예정 내년 모바일 환경에 도입…멀티모달 기술...  \n",
       "59  카카오 주요 사업 어디로 ‘카톡’ 너마저…신사업·해외 공략 차질   2010년대 중...  \n",
       "60  \"AI 콘텐츠 추천 시스템에 하이퍼클로바X 더 많이 적용…추천 고도화\" 네이버 웹사...  \n",
       "61  네이버, 실적 발표 D-1 광고 매출 성장 둔화 전망 광고수익 빈자리 커머스로 신규...  \n",
       "62  KT가 초거대 인공지능(AI) ‘믿음’을 공식 출시하며, AI B2B(기업간 거래)...  \n",
       "\n",
       "[63 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(unique_data)\n",
    "df.columns = ['제목', '언론사', '최종입력시간', '입력시간', 'URL', '본문']\n",
    "df = df.sort_values(by=['최종입력시간', '입력시간'], ascending=[False, False]).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e9bd255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.insert(0, 'new', ' ')\n",
    "df.to_excel('./project_data3.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e3ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0550d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
